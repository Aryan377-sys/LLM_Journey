{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First converting all textual data to lowercase and then **Removing HTML tag if present**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "data=pd.read_csv(\"/home/sohanx1/Downloads/Sohan/sohan/PYTHON WORKS/llm/LLM CampusX/IMDB Dataset for TextPreprocessing.csv\",encoding='utf-8')  \n",
    "data['review'].str.lower()\n",
    "def Removing_HTML_Tags(data):\n",
    "        p=re.compile('<.*?>') ## remove everything between \"< >\"\" tag even it has some text inside it \n",
    "        return p.sub(r'',data)\n",
    "data['review']=data['review'].apply(Removing_HTML_Tags) # applies 1 by 1 to every row in 'review' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Punctuation from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,time\n",
    "string.punctuation\n",
    "\n",
    "exclude=string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text\n",
    "\n",
    "text='string.With/.> .Punctuation?'\n",
    "remove_punc(text)\n",
    "\n",
    "#Timing\n",
    "\n",
    "# Since its taking a very long time we should use the below code function\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude)) \n",
    "\n",
    "start=time.time()\n",
    "print(remove_punc1(text))\n",
    "time1=time.time()-start\n",
    "print(time1*50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bot Treatment chatting words like ASAP...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chat_words={\"LMAO\":\"Laughing my ass out\",\"LOL\":\"laughing out loud\",\"ASAP\":\"As Soon As Possible\"}\n",
    "chat_text=\"Lol!, I have been looking it everywhere and you come asap\"\n",
    "def remove(text):\n",
    "   p=re.compile(\"!,.\")\n",
    "   return p.sub(r' ',text) # Using r' ' means replacing the given in compile with a whitespace\n",
    "t=remove(chat_text)\n",
    "def decode(text):\n",
    "   decoded=[]\n",
    "   for i in text.split():\n",
    "      if i.upper() in chat_words:\n",
    "         decoded.append(chat_words[i.upper()])\n",
    "      else:\n",
    "         decoded.append(i)   \n",
    "   return \" \".join(decoded)   \n",
    "decode(t)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U textblob\n",
    "! python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "incorrect_sent=\"Bihar is part of Indai and is the 7th largst stat\"\n",
    "t=TextBlob(incorrect_sent)\n",
    "t.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "not_needed=stopwords.words('english')\n",
    "text=\"Hi!, my name is Sohan Kumar and I am in pre-ultimatum year of college and love to code coz hav other option and dont do anything apart from this so I am good at it or say only thing that I do\"\n",
    "def remove(t):\n",
    "    p=re.compile(\"!,\")\n",
    "    return p.sub(r'',t)\n",
    "texto=remove(text)\n",
    "def stop(t):\n",
    "    r=[]\n",
    "    for i in t.split():\n",
    "        if i.lower() in not_needed:\n",
    "            pass\n",
    "        else:\n",
    "            r.append(i)\n",
    "    return ' '.join(r)  \n",
    "\n",
    "print(stop(texto))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Tokenization of words/sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use tokenize library from NLTK that automatically differentiate between special characters and words but has its own limits.\n",
    "# Due to this limit of tokenize we use better library that is spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "w=word_tokenize\n",
    "text=\"Hello! buddy've This's is Sohan. How have you been doing\"\n",
    "s=w(text)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc1=nlp(text)\n",
    "for i in doc1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and lemmatizer\n",
    "\n",
    "- #### **We use stemming and lemmatizer to get to the root word of the tokenized words but stemming sometimes provides us root of words that doesn't hold any value in english dictionary like *talking=talkin'***\n",
    "- #### **That's why uwe use lemmatizer which uses parts of speech to convert any words to their respective POS root,say *doing=do* when POS='verb' represented as pos='v'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer  #for stemming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lem=WordNetLemmatizer()\n",
    "\n",
    "sentence=\"Sohan is running on the road and doing things that he shouldn't be doing\"\n",
    "punctuation=\"?.:!,\"\n",
    "sen_words=nltk.word_tokenize(sentence)\n",
    "for w in sen_words:\n",
    "    if w in punctuation:\n",
    "        sen_words.remove(w)\n",
    "\n",
    "sen_words \n",
    "print(\"{0:10}{1:10}\".format(\"Words\",\"Lemma\")) # Here 0=Words and 1=Lemma\n",
    "for w in sen_words:\n",
    "    print(\"{0:20}{1:20}\".format(w,wordnet_lem.lemmatize(w,pos='a')))\n",
    "      \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sohanpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
