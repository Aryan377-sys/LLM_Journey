{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers nltk rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "- #### **Sequence to Sequence Model converts input provided by the user to a numerical values based on training on the premodel provided by Hugging Face.**\n",
    "- #### **So while generating and taking the value of temperature from user it generate words near to the tokens from pre_trained model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Generated \n",
    "- ##### **here ouput generated is in form of a dictionary inside a list**\n",
    "##### *for eg.*\n",
    "##### **[{'generated_text:'this is ....'},{'generated_text':'This is R.....'},{},{}]**\n",
    "\n",
    "- ##### **Therefore, in order to access these we use output[i]['generated_text'] where i=0,1,2,3,....,len(num_return_sequences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # pre_defined words that it recognizes and tokens it\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define the prompt and reference text\n",
    "prompt = \"This is Sohan Kumar who is...\"\n",
    "reference_text = \"This is Sohan Kumar who is the owner of FredHigh company that evaluates up to $700bn\"\n",
    "reference_tokens = reference_text.lower().split()\n",
    "\n",
    "# Dictionary to store generated text examples\n",
    "Generated_textExample = {}\n",
    "\n",
    "# Define temperatures to use\n",
    "temperatures = [0.002]\n",
    "\n",
    "for temp in temperatures:\n",
    "    output = generator(prompt, max_length=15, temperature=temp, num_return_sequences=3,do_sample=True)\n",
    "    # do_sample=True is done to return multiple sequence\n",
    "    for i,e in enumerate(output):\n",
    "        print(e['generated_text'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tokens and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) \n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT FROM USER AS REFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_from_user=\"There is a beautiful state of Bihar in India which was a once education hub for the world. \"\n",
    "reference=input_from_user.lower().split()\n",
    "X=Counter(reference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"There is a beautiful state of Bihar in India.....\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Tokens used min in both from both reference and generated text\n",
    "\n",
    "\n",
    "## Basic Token Overlap Metric\n",
    "- ##### precision=(no of common tokens from both reference and generated text/total no. of tokens in reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.333333333333334\n",
      "13.333333333333334\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for temp in [0.02,0.1,1.2]:\n",
    "    output=generator(prompt,max_length=25,temperature=temp,num_return_sequences=2,do_sample=True)\n",
    "    counts_each_temp=[]\n",
    "    sum_all_sequencefor_eachTemp=0\n",
    "    for i,each_dictionary in enumerate(output):\n",
    "        generated_text=each_dictionary['generated_text']\n",
    "        generated_tokens=tokenizer.tokenize(generated_text)\n",
    "        tokensr=[i.lower() for i in generated_tokens]\n",
    "        Y=Counter(tokensr)\n",
    "        Z= X&Y\n",
    "        intersection_counts=sum(Z.values())\n",
    "        sum_all_sequencefor_eachTemp+=intersection_counts\n",
    "    precison=(sum_all_sequencefor_eachTemp/len(reference))*100\n",
    "    print(precison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sohanpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
